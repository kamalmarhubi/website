<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kamal Marhubi</title>
    <description></description>
    <link>http://kamalmarhubi.com/</link>
    <atom:link href="http://kamalmarhubi.com/blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 18 Nov 2015 16:41:07 -0500</pubDate>
    <lastBuildDate>Wed, 18 Nov 2015 16:41:07 -0500</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Kubernetes from the ground up: the scheduler</title>
        <description>&lt;p&gt;&lt;em&gt;This is the third post in a series on &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;, the open source cluster
manager. The earlier posts were about &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/08/27/what-even-is-a-kubelet/&quot;&gt;the kubelet&lt;/a&gt;, and &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/&quot;&gt;the API
server&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It’s been a while since the last post, but I’m excited to finally finish this
one off. This is about the scheduler, which is the first part of what makes
Kubernetes Kubernetes. The scheduler’s job is to decide where in the cluster to
run our workloads. This lets us stop thinking about which host should run what,
and just declaratively say ‘I want this to be running’.&lt;/p&gt;

&lt;p&gt;When we left off last time, we were able to run a collection of containers on a
specific Kubernetes node by posting a JSON manifest to the API server. We also
got a look at the &lt;code&gt;kubectl&lt;/code&gt;, the command line client for Kubernetes, which
makes it much easier to interact with the cluster.&lt;/p&gt;

&lt;p&gt;Oh, except that until now we haven’t had a cluster, at least not in the sense
of multiple machines. In this post we’re going to change that. To follow along,
you’ll need a few machines—virtual, real, cloud, it doesn’t matter.
What does matter is&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;they are all on the same network&lt;/li&gt;
  &lt;li&gt;they all have Docker installed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ve got a few machines in the examples below: the master is &lt;code&gt;master&lt;/code&gt;, while
the nodes are &lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt;. I’m assuming they can all be reached
via their hostnames; feel free to substitute in their IPs instead!&lt;/p&gt;

&lt;h1 id=&quot;starting-the-api-server&quot;&gt;Starting the API server&lt;/h1&gt;

&lt;p&gt;We’re going to breeze through starting the API server, since it’s
all &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/#starting-the-api-server&quot;&gt;straight out of the last post&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ mkdir etcd-data
master$ docker run --volume=$PWD/etcd-data:/default.etcd \
--detach --net=host quay.io/coreos/etcd &amp;gt; etcd-container-id
master$ wget https://storage.googleapis.com/kubernetes-release/release/v1.1.1/bin/linux/amd64/kube-apiserver
master$ chmod +x kube-apiserver
master$ ./kube-apiserver \
--etcd-servers=http://127.0.0.1:2379 \
--service-cluster-ip-range=10.0.0.0/16 \
--insecure-bind-address=0.0.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only difference is we’ve added &lt;code&gt;--insecure-bind-address=0.0.0.0&lt;/code&gt;. This
allows the kubelets running on the nodes to connect to the API server remotely
without any authentication. Ordinarily, unauthenticated connections are only
allowed from localhost.&lt;/p&gt;

&lt;p&gt;Just to be clear, you &lt;em&gt;really&lt;/em&gt; don’t want to do this in production!&lt;/p&gt;

&lt;p&gt;While we’re here, let’s also get &lt;code&gt;kubectl&lt;/code&gt;, the command line client &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/#the-kubernetes-command-line-client-kubectl&quot;&gt;we looked
at in the last post&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ wget https://storage.googleapis.com/kubernetes-release/release/v1.1.1/bin/linux/amd64/kubectl
master$ chmod +x kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;launching-some-nodes&quot;&gt;Launching some nodes&lt;/h1&gt;

&lt;p&gt;This will be quick too, as we’ve done this a couple of times before. The only
difference here is that the API server isn’t running on localhost, so we need
to include its address. I’ve got two nodes, but I’ll just show this once below.
If you’re following along, do this on as many nodes as you want!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node1$ ./kubelet --api-servers=http://master:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now back on &lt;code&gt;master&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl get nodes
NAME      LABELS                         STATUS    AGE
node1     kubernetes.io/hostname=node1   Ready     2m
node2     kubernetes.io/hostname=node2   Ready     4s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Excellent.&lt;/p&gt;

&lt;h1 id=&quot;running-something-on-the-cluster&quot;&gt;Running something on the cluster&lt;/h1&gt;

&lt;p&gt;Kubernetes runs &lt;em&gt;pods&lt;/em&gt;, which are collections of containers that execute
together.  To start, we’ll create a pod and specify which node it should run
on.&lt;/p&gt;

&lt;p&gt;We’ll continue running our nginx example pod from the earlier posts. Get &lt;a href=&quot;https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/03-the-scheduler/nginx-with-nodename.yaml&quot;&gt;the
pod manifest&lt;/a&gt;, which specifies which containers to run. We
specify the node to run on by setting the &lt;code&gt;nodeName&lt;/code&gt; field. Edit the file and
set it to run on one of your nodes.  I picked &lt;code&gt;node2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ wget https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/03-the-scheduler/nginx-with-nodename.yaml
master$ $EDITOR nginx-with-nodename.yaml  # edit the nodeName field to match a node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create the pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl create --filename nginx-with-nodename.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check with &lt;code&gt;kubectl get pods&lt;/code&gt; we see that it got picked up. If you’re
quicker than me, you might catch it in the &lt;code&gt;Pending&lt;/code&gt; state, before the kubelet
starts it, but it should end up &lt;code&gt;Running&lt;/code&gt; fairly quickly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl get pods
NAME                  READY     STATUS    RESTARTS   AGE
nginx-with-nodename   2/2       Running   0          7s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just to be sure it’s actually on &lt;code&gt;node2&lt;/code&gt; as we said, we can &lt;code&gt;kubectl describe&lt;/code&gt;
the pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl describe pods/nginx-with-nodename | grep ^Node
Node:                           node2/10.240.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can break down what happened here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;initially, the kubelets on each node are watching the API server for pods
they are meant to be running&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; created a pod on the API server that’s meant to run on &lt;code&gt;node2&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;the kubelet on &lt;code&gt;node2&lt;/code&gt; noticed the new pod, and so started running it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can also try &lt;a href=&quot;https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/03-the-scheduler/nginx-without-nodename.yaml&quot;&gt;a pod manifest&lt;/a&gt; that doesn’t specify a
node to run on. In our current setup, this pod will forever sit in the
&lt;code&gt;Pending&lt;/code&gt; state. Let’s try anyway:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ wget https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/03-the-scheduler/nginx-without-nodename.yaml
master$ ./kubectl create --filename nginx-without-nodename.yaml
master$ ./kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-with-nodename      2/2       Running   0          3m
nginx-without-nodename   0/2       Pending   0          20s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even if you take a break and read the internet for 15 minutes, it’ll still be
there, &lt;code&gt;Pending&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-with-nodename      2/2       Running   0          18m
nginx-without-nodename   0/2       Pending   0          15m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Breaking it down in the same way:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;initially, the kubelets on each node are watching the API server for pods
they are meant to be running&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; created a pod on the API server without specifying which node to
run on&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
  &lt;li&gt;… yeah, nothing’s going to happen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-scheduler&quot;&gt;The scheduler&lt;/h1&gt;

&lt;p&gt;This is where the sheduler comes in: its job is to take pods that aren’t bound
to a node, and assign them one. Once the pod has a node assigned, the normal
behavior of the kubelet kicks in, and the pod gets started.&lt;/p&gt;

&lt;p&gt;Let’s get the scheduler binary and start it running on &lt;code&gt;master&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ wget https://storage.googleapis.com/kubernetes-release/release/v1.1.1/bin/linux/amd64/kube-scheduler
master$ chmod +x kubectl
master$ ./kube-scheduler --master=hp://localhost:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not long after starting the scheduler, the &lt;code&gt;nginx-without-nodename&lt;/code&gt; pod should
get assigned a node and start running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-with-nodename      2/2       Running   0          1h
nginx-without-nodename   2/2       Running   0          1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we &lt;code&gt;describe&lt;/code&gt; it, we can see which node it got scheduled on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl describe pods/nginx-without-nodename | grep ^Node
Node:                           node1/10.240.0.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It ended up on &lt;code&gt;node1&lt;/code&gt;! The scheduler tries to spread out pods evenly across
the nodes we have available, so that makes sense. If you’re interested in more
about how the scheduler places pods, there’s a really good &lt;a href=&quot;http://stackoverflow.com/a/28874577&quot;&gt;Stack
Overflow&lt;/a&gt; answer with some details.&lt;/p&gt;

&lt;p&gt;We can also get a list of ‘events’ related to the pod. These are state changes
through the pods lifetime:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl describe pods/nginx-without-nodename | grep -A5 ^Events
Events:
  FirstSeen     LastSeen        Count   From            SubobjectPath                           Reason                  Message
  ─────────     ────────        ─────   ────            ─────────────                           ──────                  ───────
  25m           25m             1       {scheduler }                                            Scheduled               Successfully assigned nginx-without-nodename to node1
  23m           23m             1       {kubelet node1} implicitly required container POD       Pulling                 Pulling image &quot;gcr.io/google_containers/pause:0.8.0&quot;
  23m           23m             1       {kubelet node1} implicitly required container POD       Pulled                  Successfully pulled image &quot;gcr.io/google_containers/pause:0.8.0&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first one shows it getting scheduled, then others are related to the pod
starting up on the node.&lt;/p&gt;

&lt;p&gt;At this point, if you create another pod without specifying a node for it to
run on, the scheduler will place it right away. Try it out!&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h1&gt;

&lt;p&gt;So now we are able to declaratively specify workloads, and get them scheduled
across our cluster, which is great! But if we actually try connecting to the
nginx servers we have running, we’ll see we have a little problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;master$ ./kubectl describe pods/nginx-with-nodename | grep ^IP
IP:                             172.17.0.2
master$ curl http://172.17.0.2
curl: (7) Failed to connect to 172.17.0.2 port 80: No route to host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This pod is running on &lt;code&gt;node2&lt;/code&gt;. If we go over to that machine,
we get through:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node2$ curl --stderr /dev/null http://172.17.0.2 | head -4
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But our other node can’t reach it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node1$ curl http://172.17.0.2
curl: (7) Failed to connect to 172.17.0.2 port 80: No route to host
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the next post, we’ll take a little detour into Kubernetes networking, and
make it possible for containers to talk to each other over the network.&lt;/p&gt;
</description>
        <pubDate>Tue, 17 Nov 2015 17:41:57 -0500</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/11/17/kubernetes-from-the-ground-up-the-scheduler/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/11/17/kubernetes-from-the-ground-up-the-scheduler/</guid>
        
        
      </item>
    
      <item>
        <title>Eliminating branches in Rust for fun... but not much profit</title>
        <description>&lt;p&gt;Last week, I nerd-sniped myself after reading a &lt;a href=&quot;http://m4rw3r.github.io/parser-combinator-experiments-part-3/&quot;&gt;blog post&lt;/a&gt; about
performance of parser combinators in Rust, and the associated &lt;a href=&quot;https://www.reddit.com/r/rust/comments/3k0d0d/parser_combinator_experiments_part_3_performance/&quot;&gt;Reddit
discussion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The benchmark in the post was parsing a bunch of HTTP requests. The focus was
on improving performance of a function determining if a character formed part
of a token. In the post, it was changed from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn is_token(c: u8) -&amp;gt; bool {
    c &amp;lt; 128 &amp;amp;&amp;amp; c &amp;gt; 31 &amp;amp;&amp;amp; b&quot;()&amp;lt;&amp;gt;@,;:\\\&quot;/[]?={} \t&quot;.iter()
                           .position(|&amp;amp;i| i == c).is_none()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which iterates over a list of characters, to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fn is_token(c: u8) -&amp;gt; bool {
    // roughly follows the order of ascii chars: &quot;\&quot;(),/:;&amp;lt;=&amp;gt;?@[\\]{} \t&quot;
    c &amp;lt; 128 &amp;amp;&amp;amp; c &amp;gt; 32 &amp;amp;&amp;amp; c != b&#39;\t&#39; &amp;amp;&amp;amp; c != b&#39;&quot;&#39; &amp;amp;&amp;amp; c != b&#39;(&#39; &amp;amp;&amp;amp; c != b&#39;)&#39; &amp;amp;&amp;amp;
        c != b&#39;,&#39; &amp;amp;&amp;amp; c != b&#39;/&#39; &amp;amp;&amp;amp; !(c &amp;gt; 57 &amp;amp;&amp;amp; c &amp;lt; 65) &amp;amp;&amp;amp; !(c &amp;gt; 90 &amp;amp;&amp;amp; c &amp;lt; 94) &amp;amp;&amp;amp;
        c != b&#39;{&#39; &amp;amp;&amp;amp; c != b&#39;}&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which unrolls the loop into a big boolean conjunction.&lt;/p&gt;

&lt;p&gt;On my machine, the benchmark with the original version takes about 106
microseconds to parse a text file containing about 50 HTTP requests. The newer
version takes 73 microseconds, which is about 30% faster.&lt;/p&gt;

&lt;p&gt;After reading this, I started thinking something like “I know about CPUs, and
branches are bad because of mispredictions or something”, and “LOOK &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; AT
&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; ALL &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; THOSE &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; BRANCHES!”, and imagining fame and fortune for making
it even faster.&lt;/p&gt;

&lt;p&gt;This led me down a path of trying to replace that function with a
straight-through series of bitwise operations. First off, let’s unpack what’s
actually being checked in this function:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;c &amp;lt; 128&lt;/code&gt;: that &lt;code&gt;c&lt;/code&gt; represents asn ASCII character&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;c &amp;gt; 32&lt;/code&gt;: that &lt;code&gt;c&lt;/code&gt; is not a control character or space&lt;/li&gt;
  &lt;li&gt;all the &lt;code&gt;!=&lt;/code&gt; comparisons: that &lt;code&gt;c&lt;/code&gt; isn’t any of those characters&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;!(c &amp;gt; 57 &amp;amp;&amp;amp; c &amp;lt; 65)&lt;/code&gt;: that &lt;code&gt;c&lt;/code&gt; is none of &lt;code&gt;:;&amp;lt;=&amp;gt;?@&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;!(c &amp;gt; 90 &amp;amp;&amp;amp; c &amp;lt; 94)&lt;/code&gt;: that &lt;code&gt;c&lt;/code&gt; is none of &lt;code&gt;[\]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;eliminate-almost-all-the-branches&quot;&gt;Eliminate (almost) ALL THE BRANCHES&lt;/h1&gt;

&lt;p&gt;My basic idea was to replace all the equality checks with XOR, and all the
boolean ands with bitwise ands. Boolean ands (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;) and ors (&lt;code&gt;||&lt;/code&gt;) have short
circuiting semantics: their right hand sides are only evaluated if necessary.
This means that &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; has to be implemented as a conditional jump to avoid
evaluating the right hand side if the left hand side is false.&lt;/p&gt;

&lt;p&gt;To check I was making any sense, I used the fantastic Godbolt interactive
compiler, which &lt;a href=&quot;http://rust.godbolt.org/&quot;&gt;has Rust support&lt;/a&gt;. This confirmed that at least
some of the boolean ands were compiling to conditional jumps, which was enough
for me to go down the bitwise operation path.&lt;/p&gt;

&lt;p&gt;After spending a bunch of time on translating it to be purely bitwise
operations, I ended up with this performance result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test bench_http ... bench:     230,030 ns/iter (+/- 6,303)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;About three times slower. This was… disappointing. Especially because along
the way, a broken benchmark setup had me convinced I sped things up by 60%!&lt;/p&gt;

&lt;h1 id=&quot;why-so-slow&quot;&gt;Why so slow?&lt;/h1&gt;

&lt;p&gt;I talked about my results with Dan Luu, since he’s written about this kind of
thing before. He mentioned that on Haswell, the missed branch prediction
penalty is about 14 cycles. Optimally organised bitwise operations can be
about 4 per cycle.&lt;/p&gt;

&lt;p&gt;The original version had about 30 instructions, of which about 6 were branches.
Assuming one instruction per cycle, a really bad branch predictor miss rate
like 50% would be somewhere around 75 cycles.  The bitwise version had about
130 instructions, of which one was a branch. Assume the branch predictor is
always right, that still puts me at 40+ cycles if 3 are in parallel.&lt;/p&gt;

&lt;p&gt;In summary, unless the branch predictor was abysmally bad at this code, or I
was amazingly excellent at organizing the bitwise operations, there was no
strong reason to expect my my bitwise version to be faster.&lt;/p&gt;

&lt;p&gt;This got me to run &lt;code&gt;perf stat&lt;/code&gt; on the benchmark to get a look at the branch
prediction hit rate. This turned out to be above 99.5%, which is far from
‘abysmal’.&lt;/p&gt;

&lt;h1 id=&quot;another-approach-a-lookup-table&quot;&gt;Another approach: a lookup table&lt;/h1&gt;

&lt;p&gt;The discussion also suggested implementing this as a lookup table. This would
reduce &lt;code&gt;is_token&lt;/code&gt; to a single load which can be inlined wherever it’s used.
Rust represents boolean values as bytes, so the table will be 256 bytes which
should mean we’ll rarely have to go too far down the cache hierarchy.&lt;/p&gt;

&lt;p&gt;Indeed, implementing that gave a modest 6-7%  performance improvement over the
version that sent me off on this investigation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test bench_http ... bench:      67,890 ns/iter (+/- 1,913)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So at least there was a somewhat satisfactory result at the end!&lt;/p&gt;

&lt;h1 id=&quot;the-moral-of-the-story-validate-assumptions-and-know-more-numbers&quot;&gt;The moral of the story: validate assumptions, and know more numbers!&lt;/h1&gt;

&lt;p&gt;There were a couple of big takeaways for me. The first is I could have saved
myself a whole bunch of time if I’d investigated the branch prediction
performance before embarking on this adventure. Instead of assuming the
branching was slowing things down, I would know that it probably wasn’t.&lt;/p&gt;

&lt;p&gt;The other was knowing some performance numbers offhand could have helped a lot
here. I have a rough idea of L1 / L2 latency, and main memory latency. But I
had no idea how many bitwise operations to expect per cycle, or how bad a
branch misprediction was. I’ve now added these to my collection, but knowing a
few more rough figures like that would certainly be good!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Thanks to Dan Luu, Anton Dubrau, and David Turner for useful and interesting
discussions, and to Julia Evans for reading drafts of this post.&lt;/em&gt;&lt;/p&gt;

&lt;!--

# How I implemented a bitwise version
Here&#39;s how I built up my bitwise version. For my own sanity, and ease of
translation, I decided to use `0x0` as false and `0x1` as true. Implement first
and improve afterwards if it seems hopeful. I started off using the fact that
`x ^ y` is all zeros if and only if `x` and `y` are equal. Or, put another way,
if `x != y` at least one bit in `x ^ y` is set. This will be handy since most
of our comparisons are checking inequality with a fixed byte. This lets us
write

~~~
fn eq(x: u8, y: u8) -&gt; u8 {
    not(is_non_zero(x ^ y))
~~~

We need `is_non_zero` to return `0x1` or `0x0`; we&#39;ll get to that in a second.
Once it does, the `not` operation is easy: just XOR with `0x1`:

~~~
fn not(x: u8) -&gt; u8 {
    x ^ 1
}
~~~

As for this `is_non_zero` operation, that&#39;s a bit trickier. Fortunately, [the
internet][stackoverflow]! Here&#39;s the implementation:

~~~
fn is_non_zero(x: u8) -&gt; u8 {
    ((x | ((!x).wrapping_add(1))) &gt;&gt; 7)
}
~~~

What we want to do is return `0x1` if any bit of `x` is set, and otherwise
return `0x0`. We&#39;ll work out how this works from the outside in. If we can get the most signicant bit set in The way this works is to force the most significant bit to be set
if any bit is set. Then the `&gt;&gt; 7` will bring the most significant bit down to
be the least significant bit, and we&#39;ll have 

Finally, we&#39;ve got comparisons with `128` and `32`. Both of these are powers of
two, which makes it a bit easier to check! If the most significant bit is set,
then the number is greater than or equal to `128`, so we bitwise and with `128`
and check if that&#39;s non-zero:


~~~
pub fn ge_128(x: u8) -&gt; u8 {
    is_non_zero(128 &amp; x)
}
~~~

For checking a number is less than `32`, we need to check that none of the upper 3 bits are set. For this, we can bitwise and with the bitwise compelement of 31, which has all the lower 5 bits set.

~~~
pub fn lt_32(x: u8) -&gt; u8 {
    not(is_non_zero(!0x1f &amp; x))
}
~~~

Finally, we need a way to go from `0x1` and `0x0` to `true` and `false`. Since we only have

This let me put the whole thing together:


[stackoverflow]: http://stackoverflow.com/questions/3912112/check-if-a-number-is-non-zero-using-bitwise-operators-in-c
--&gt;
</description>
        <pubDate>Tue, 15 Sep 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/09/15/eliminating-branches-in-rust-for-fun-but-not-much-profit/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/09/15/eliminating-branches-in-rust-for-fun-but-not-much-profit/</guid>
        
        
      </item>
    
      <item>
        <title>Kubernetes from the ground up: the API server</title>
        <description>&lt;p&gt;&lt;em&gt;This is the second post in a series on &lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;, the open source cluster
manager. The first post was &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/08/27/what-even-is-a-kubelet/&quot;&gt;about the kubelet&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://kamalmarhubi.com/blog/2015/08/27/what-even-is-a-kubelet/&quot;&gt;Last time&lt;/a&gt; we took a look at the kubelet, Kubernetes’
container-focused process watcher. The kubelet runs pods, which are collections
of containers that share an IP and some volumes. In the post, we gave it pods
to run by putting pod manifest files in directory it watched. This was a great
way to understand the core purpose of the kubelet.  In a Kubernetes cluster,
however, a kubelet will get most its pods to run from the Kubernetes API
server.&lt;/p&gt;

&lt;p&gt;Kubernetes stores all its cluster state in &lt;a href=&quot;https://github.com/coreos/etcd&quot;&gt;etcd&lt;/a&gt;, a distributed data store with
a strong consistency model. This state includes what nodes exist in the cluster,
what pods should be running, which nodes they are running on, and a whole lot
more. The API server is the only Kubernetes component that connects to etcd; all
the other components must go through the API server to work with cluster state.
In this post we’ll look at the API server, and its interaction with the kubelet.&lt;/p&gt;

&lt;h1 id=&quot;starting-the-api-server&quot;&gt;Starting the API server&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;You may need to use &lt;code&gt;sudo&lt;/code&gt; on some commands, depending on your setup.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First off, we’re going to need etcd running. Luckily this is as easy as creating
a directory for it to store its state and starting it with Docker. We’ll also
save the Docker container ID so we can stop the container later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir etcd-data
$ docker run --volume=$PWD/etcd-data:/default.etcd \
--detach --net=host quay.io/coreos/etcd &amp;gt; etcd-container-id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use host networking so that the API server can talk to it at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next we’ll want the API server binary:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://storage.googleapis.com/kubernetes-release/release/v1.0.3/bin/linux/amd64/kube-apiserver
$ chmod +x kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can start it up. It needs to know where the etcd server is, as well as
the service cluster IP range. We’ll save talking about what the IP range is for
a later post that will go into Kubernetes’ services and networking. For now
we’ll just provide &lt;code&gt;10.0.0.0/16&lt;/code&gt; so that the API server starts up without
shouting at us!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kube-apiserver \
--etcd-servers=http://127.0.0.1:2379 \
--service-cluster-ip-range=10.0.0.0/16 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now &lt;code&gt;curl&lt;/code&gt; around and check a few things out. First off, we can get a
list of nodes in our cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://localhost:8080/api/v1/nodes
{
  &quot;kind&quot;: &quot;NodeList&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    &quot;selfLink&quot;: &quot;/api/v1/nodes&quot;,
    &quot;resourceVersion&quot;: &quot;150&quot;
  },
  &quot;items&quot;: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not surprisingly, there aren’t any yet.&lt;/p&gt;

&lt;p&gt;As a quick note on other fields in the response: the &lt;code&gt;kind&lt;/code&gt; and &lt;code&gt;apiVersion&lt;/code&gt;
are giving information about the API version and type of response we got. The
&lt;code&gt;selfLink&lt;/code&gt; field is a canonical link for the resource in the response. The
&lt;code&gt;resourceVersion&lt;/code&gt; is used for concurrency control. Clients send it back when
they are changing a resource, and the server can determine if there was a
conflicting write to the same resource in the meantime.&lt;/p&gt;

&lt;p&gt;All that is to say: right now we only care about the &lt;code&gt;items&lt;/code&gt; field. We can use
the incredibly handy &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt; utility to just get at the items. We’ll use &lt;code&gt;jq&lt;/code&gt;
to cut out noisy bits of responses throughout this post. For example, we can
look at what pods our cluster is running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://localhost:8080/api/v1/pods | jq &#39;.items&#39;
[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No surprises there, either!&lt;/p&gt;

&lt;h1 id=&quot;adding-a-node&quot;&gt;Adding a node&lt;/h1&gt;

&lt;p&gt;In the last post, we had the kubelet watching for pod manifest files in a
directory we gave it via the &lt;code&gt;--config&lt;/code&gt; flag. This time we’ll have it get pod
manifests from the API server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubelet --api-servers=127.0.0.1:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a kubelet starts up, it registers itself as a node with the API server and
starts watching for pods to run. This is really great, because it means that
when we get to running a multinode cluster, we can add nodes without having to
reconfigure the API server.&lt;/p&gt;

&lt;p&gt;We can check that the API server knows about our node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://localhost:8080/api/v1/nodes/ \
| jq &#39;.items&#39; | head
[
  {
    &quot;metadata&quot;: {
      &quot;name&quot;: &quot;awesome-node&quot;,
      &quot;selfLink&quot;: &quot;/api/v1/nodes/awesome-node&quot;,
      &quot;uid&quot;: &quot;6811f7b0-5181-11e5-b364-68f7288bdc45&quot;,
      &quot;resourceVersion&quot;: &quot;246&quot;,
      &quot;creationTimestamp&quot;: &quot;2015-09-02T14:46:34Z&quot;,
      &quot;labels&quot;: {
        &quot;kubernetes.io/hostname&quot;: &quot;awesome-node&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have a one-node cluster!&lt;/p&gt;

&lt;h1 id=&quot;running-a-pod-via-the-api-server&quot;&gt;Running a pod via the API server&lt;/h1&gt;

&lt;p&gt;Let’s run our nginx example from the last post:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/01-the-kubelet/nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a complete Kubernetes cluster, the scheduler will decide which node to run a
pod on. For now, we’ve only got the API server and a kubelet, so we’ll have to
specify it ourselves. To do this, we need to add a &lt;code&gt;nodeName&lt;/code&gt; to the spec with
the node’s &lt;code&gt;name&lt;/code&gt; from above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sed --in-place &#39;/spec:/a\ \ nodeName: awesome-node&#39; nginx.yaml
$ head nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: awesome-node
  containers:
  - name: nginx
    image: nginx
    ports:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the &lt;code&gt;nodeName&lt;/code&gt; configured, we’re almost ready to send the pod manifest to
the API server. Unfortunately, it only speaks JSON so we have to convert our YAML to
JSON:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ruby -ryaml -rjson \
-e &#39;puts JSON.pretty_generate(YAML.load(ARGF))&#39; &amp;lt; nginx.yaml &amp;gt; nginx.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, just download the &lt;a href=&quot;https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/02-the-api-server/nginx.json&quot;&gt;JSON file&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/02-the-api-server/nginx.yaml&quot;&gt;YAML
file&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/02-the-api-server/nginx.json
$ wget https://raw.githubusercontent.com/kamalmarhubi/kubernetes-from-the-ground-up/master/02-the-api-server/nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then edit the files so that the &lt;code&gt;nodeName&lt;/code&gt; matches your hostname.&lt;/p&gt;

&lt;p&gt;Now we can post the JSON pod manifest to the API server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl \
--stderr /dev/null \
--request POST http://localhost:8080/api/v1/namespaces/default/pods \
--data @nginx.json | jq &#39;del(.spec.containers, .spec.volumes)&#39;
{
  &quot;kind&quot;: &quot;Pod&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx&quot;,
    &quot;namespace&quot;: &quot;default&quot;,
    &quot;selfLink&quot;: &quot;/api/v1/namespaces/default/pods/nginx&quot;,
    &quot;uid&quot;: &quot;28aa5a55-5194-11e5-b364-68f7288bdc45&quot;,
    &quot;resourceVersion&quot;: &quot;1365&quot;,
    &quot;creationTimestamp&quot;: &quot;2015-09-02T17:00:48Z&quot;
  },
  &quot;spec&quot;: {
    &quot;restartPolicy&quot;: &quot;Always&quot;,
    &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
    &quot;nodeName&quot;: &quot;awesome-node&quot;
  },
  &quot;status&quot;: {
    &quot;phase&quot;: &quot;Pending&quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a short wait, the kubelet should have started the pod. We can check this
by making a GET request:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://localhost:8080/api/v1/namespaces/default/pods \
| jq &#39;.items[] | { name: .metadata.name, status: .status} | del(.status.containerStatuses)&#39;
{
  &quot;name&quot;: &quot;nginx&quot;,
  &quot;status&quot;: {
    &quot;phase&quot;: &quot;Running&quot;,
    &quot;conditions&quot;: [
      {
        &quot;type&quot;: &quot;Ready&quot;,
        &quot;status&quot;: &quot;True&quot;
      }
    ],
    &quot;hostIP&quot;: &quot;127.0.1.1&quot;,
    &quot;podIP&quot;: &quot;172.17.0.37&quot;,
    &quot;startTime&quot;: &quot;2015-09-02T18:00:00Z&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The pod is up, and it’s been assigned the IP &lt;code&gt;172.17.0.37&lt;/code&gt; by Docker. Docker
networking is really quite interesting, and well worth reading about. A good
place to start is &lt;a href=&quot;https://docs.docker.com/articles/networking/&quot;&gt;the network configuration article&lt;/a&gt; in the Docker
documentation.&lt;/p&gt;

&lt;p&gt;Let’s check that nginx is reachable at that IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://172.17.0.37 | head -4
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Excellent!&lt;/p&gt;

&lt;h1 id=&quot;the-kubernetes-command-line-client-kubectl&quot;&gt;The Kubernetes command line client: kubectl&lt;/h1&gt;

&lt;p&gt;While it’s great to know that the API server speaks a fairly intelligible REST
dialect, talking to it directly with &lt;code&gt;curl&lt;/code&gt; and using &lt;code&gt;jq&lt;/code&gt; to filter the
responses isn’t the best user experience. This is a great point to pause and
introduce the command line client&lt;a href=&quot;http://kubernetes.io/v1.0/docs/user-guide/kubectl/kubectl.html&quot;&gt; &lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt;, which we’ll use
throughout the rest of this series. It will make things &lt;strong&gt;much&lt;/strong&gt; nicer!&lt;/p&gt;

&lt;p&gt;First off, let’s download the client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://storage.googleapis.com/kubernetes-release/release/v1.0.3/bin/linux/amd64/kubectl
$ chmod +x kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can get the list of nodes and see what pods are running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubectl get nodes
NAME      LABELS                      STATUS
awesome-node        kubernetes.io/hostname=awesome-node   Ready
$ ./kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
nginx     2/2       Running   0          28m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much easier and prettier! Creating pods is also easier with &lt;code&gt;kubectl&lt;/code&gt;. Let’s
create a copy of the nginx pod manifest with a different name.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sed &#39;s/^  name:.*/  name: nginx-the-second/&#39; nginx.yaml &amp;gt; nginx2.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can use &lt;code&gt;kubectl create&lt;/code&gt; to start another copy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubectl create --filename nginx2.yaml
pods/nginx-the-second
$ ./kubectl get pods
NAME               READY     STATUS    RESTARTS   AGE
nginx              2/2       Running   0          1h
nginx-the-second   0/2       Running   0          6s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ve got our second nginx pod running, but it reports &lt;code&gt;0/2&lt;/code&gt; containers
running. Let’s give it a bit and try again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubectl get pods
NAME               READY     STATUS    RESTARTS   AGE
nginx              2/2       Running   0          1h
nginx-the-second   2/2       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also use &lt;code&gt;kubectl describe&lt;/code&gt; to get at more detailed information on the
pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubectl describe pods/nginx-the-second | head
Name:                           nginx-the-second
Namespace:                      default
Image(s):                       nginx,busybox
Node:                           awesome-node/127.0.1.1
Labels:                         &amp;lt;none&amp;gt;
Status:                         Running
Reason:
Message:
IP:                             172.17.0.38
Replication Controllers:        &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And just to be sure, we can check that this pod’s nginx is also up and serving
requests at the pod’s IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://172.17.0.38 | head -4
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! So now we’ve seen what it’s like to start a server in Kubernetes using
the command line client. We’ve still got a little way to go before this is a
full-blown Kubernetes cluster, but we are inching closer. Next time we’ll bring
in the scheduler and add a couple more nodes into the mix.&lt;/p&gt;

&lt;p&gt;For now, let’s just tear everything down:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./kubectl delete pods/nginx pods/nginx-the-second
pods/nginx
pods/nginx-the-second
$ ./kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
$ docker stop $(cat etcd-container-id)
$ sleep 20  # wait for the Kubelet to stop all the containers
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Thanks to Johannes Alkjær, Julia Evans, Ray Bejjani, and Tavish Armstrong for
reviewing drafts of this post.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2015 12:02:47 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/09/06/kubernetes-from-the-ground-up-the-api-server/</guid>
        
        
      </item>
    
      <item>
        <title>What even is a kubelet?</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; is Google’s open source, container-focused cluster management
thing.  I see it as their attempt to tell everyone how they think containers
and clusters fit together. The Kubernetes documentation is quite good, but it’s
divided up in a way that makes it great as a reference. I want to understand
both the concepts Kubernetes introduces, and the components that make up a
Kubernetes cluster, and I want to learn by doing. I’m planning to build up a
cluster from scratch, documenting the moving parts and concepts as I go.&lt;/p&gt;

&lt;p&gt;I’ll start of with a look at the &lt;em&gt;&lt;a href=&quot;http://kubernetes.io/v1.0/docs/admin/kubelet.html&quot;&gt;kubelet&lt;/a&gt;&lt;/em&gt;, which is the lowest level component
in Kubernetes. It’s responsible for what’s running on an individual machine.
You can think of it as a process watcher like &lt;a href=&quot;http://supervisord.org/&quot;&gt;supervisord&lt;/a&gt;, but focused on
running containers. It has one job: given a set of containers to run, make sure
they are all running.&lt;/p&gt;

&lt;h1 id=&quot;kubelets-run-pods&quot;&gt;Kubelets run pods&lt;/h1&gt;

&lt;p&gt;The unit of execution that Kubernetes works with is the &lt;em&gt;&lt;a href=&quot;http://kubernetes.io/v1.0/docs/user-guide/pods.html&quot;&gt;pod&lt;/a&gt;&lt;/em&gt;. A pod is a
collection of containers that share some resources: they have a single IP, and
can share volumes. For example, a web server pod could have a container for the
server itself, and a container that tails the logs and ships them off to your
logging or metrics infrastructure.&lt;/p&gt;

&lt;p&gt;Pods are defined by a JSON or YAML file called a pod manifest. A simple one
with one container looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-coderay&quot;&gt;&lt;div class=&quot;CodeRay&quot;&gt;
  &lt;div class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n1&quot; name=&quot;n1&quot;&gt;1&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;apiVersion&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;v1&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n2&quot; name=&quot;n2&quot;&gt;2&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;kind&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;Pod&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n3&quot; name=&quot;n3&quot;&gt;3&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;metadata&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n4&quot; name=&quot;n4&quot;&gt;4&lt;/a&gt;&lt;/span&gt;  &lt;span style=&quot;color:#606&quot;&gt;name&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n5&quot; name=&quot;n5&quot;&gt;5&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;spec&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n6&quot; name=&quot;n6&quot;&gt;6&lt;/a&gt;&lt;/span&gt;  &lt;span style=&quot;color:#606&quot;&gt;containers&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n7&quot; name=&quot;n7&quot;&gt;7&lt;/a&gt;&lt;/span&gt;  - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;name: nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n8&quot; name=&quot;n8&quot;&gt;8&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;image&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n9&quot; name=&quot;n9&quot;&gt;9&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;ports&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;strong&gt;&lt;a href=&quot;#n10&quot; name=&quot;n10&quot;&gt;10&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;    - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;containerPort: 80&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The container’s &lt;code&gt;image&lt;/code&gt; is a Docker image name. The &lt;code&gt;containerPort&lt;/code&gt; exposes
that port from the nginx container so we can connect to the nginx server at the
pod’s IP. By default, the &lt;a href=&quot;http://docs.docker.com/reference/builder/#entrypoint&quot;&gt;entrypoint&lt;/a&gt; defined in the image is what will run; in
the nginx image, that’s the nginx server.&lt;/p&gt;

&lt;p&gt;Let’s add a log truncator container to this pod. This will take care of the
nginx access log, truncating it every 10 seconds—who needs those anyway? To do
this, we’ll need nginx to write its logs to a volume that can be shared to the
log truncator. We’ll set this volume up as an &lt;code&gt;emptyDir&lt;/code&gt; volume: it will start
off as an empty directory when the pod starts, and be cleaned up when the pod
exits, but will persist across restarts of the component containers.&lt;/p&gt;

&lt;p&gt;Here’s the updated pod manifest:&lt;/p&gt;

&lt;div class=&quot;highlighter-coderay&quot;&gt;&lt;div class=&quot;CodeRay&quot;&gt;
  &lt;div class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n1&quot; name=&quot;n1&quot;&gt;1&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;apiVersion&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;v1&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n2&quot; name=&quot;n2&quot;&gt;2&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;kind&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;Pod&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n3&quot; name=&quot;n3&quot;&gt;3&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;metadata&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n4&quot; name=&quot;n4&quot;&gt;4&lt;/a&gt;&lt;/span&gt;  &lt;span style=&quot;color:#606&quot;&gt;name&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n5&quot; name=&quot;n5&quot;&gt;5&lt;/a&gt;&lt;/span&gt;&lt;span style=&quot;color:#606&quot;&gt;spec&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n6&quot; name=&quot;n6&quot;&gt;6&lt;/a&gt;&lt;/span&gt;  &lt;span style=&quot;color:#606&quot;&gt;containers&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n7&quot; name=&quot;n7&quot;&gt;7&lt;/a&gt;&lt;/span&gt;  - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;name: nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n8&quot; name=&quot;n8&quot;&gt;8&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;image&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt; &lt;a href=&quot;#n9&quot; name=&quot;n9&quot;&gt;9&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;ports&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;strong&gt;&lt;a href=&quot;#n10&quot; name=&quot;n10&quot;&gt;10&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;    - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;containerPort: 80&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n11&quot; name=&quot;n11&quot;&gt;11&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;volumeMounts&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n12&quot; name=&quot;n12&quot;&gt;12&lt;/a&gt;&lt;/span&gt;    - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;mountPath: /var/log/nginx&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n13&quot; name=&quot;n13&quot;&gt;13&lt;/a&gt;&lt;/span&gt;      &lt;span style=&quot;color:#606&quot;&gt;name&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx-logs&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n14&quot; name=&quot;n14&quot;&gt;14&lt;/a&gt;&lt;/span&gt;  - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;name: log-truncator&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n15&quot; name=&quot;n15&quot;&gt;15&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;image&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;busybox&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n16&quot; name=&quot;n16&quot;&gt;16&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;command&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n17&quot; name=&quot;n17&quot;&gt;17&lt;/a&gt;&lt;/span&gt;    - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;/bin/sh&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n18&quot; name=&quot;n18&quot;&gt;18&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;args&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;[-c, &#39;while true; do cat /dev/null &amp;gt; /logdir/access.log; sleep 10; done&#39;]&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n19&quot; name=&quot;n19&quot;&gt;19&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;volumeMounts&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;strong&gt;&lt;a href=&quot;#n20&quot; name=&quot;n20&quot;&gt;20&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;    - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;mountPath: /logdir&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n21&quot; name=&quot;n21&quot;&gt;21&lt;/a&gt;&lt;/span&gt;      &lt;span style=&quot;color:#606&quot;&gt;name&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;nginx-logs&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n22&quot; name=&quot;n22&quot;&gt;22&lt;/a&gt;&lt;/span&gt;  &lt;span style=&quot;color:#606&quot;&gt;volumes&lt;/span&gt;:
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n23&quot; name=&quot;n23&quot;&gt;23&lt;/a&gt;&lt;/span&gt;  - &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;name: nginx-logs&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;line-numbers&quot;&gt;&lt;a href=&quot;#n24&quot; name=&quot;n24&quot;&gt;24&lt;/a&gt;&lt;/span&gt;    &lt;span style=&quot;color:#606&quot;&gt;emptyDir&lt;/span&gt;: &lt;span style=&quot;background-color:hsla(0,100%,50%,0.05)&quot;&gt;&lt;span style=&quot;color:#D20&quot;&gt;{}&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;We’ve added an &lt;code&gt;emptyDir&lt;/code&gt; volume named &lt;code&gt;nginx-logs&lt;/code&gt;. nginx writes its logs at
/var/log/nginx, so we mount that volume at that location in the &lt;code&gt;nginx&lt;/code&gt;
container. For the &lt;code&gt;log-truncator&lt;/code&gt; container, we’re using the &lt;a href=&quot;https://hub.docker.com/_/busybox/&quot;&gt;busybox&lt;/a&gt; image.
It’s a tiny Linux command line environment, which provides everything we need
for a robust log truncator. Inside that container, we’ve mounted the
&lt;code&gt;nginx-logs&lt;/code&gt; volume at &lt;code&gt;/logdir&lt;/code&gt;. We set its &lt;code&gt;command&lt;/code&gt; and &lt;code&gt;args&lt;/code&gt; up to run a
shell loop that truncates the log file every 10 seconds.&lt;/p&gt;

&lt;p&gt;Now we’ve got this paragon of production infrastructure configured, it’s time
to run it!&lt;/p&gt;

&lt;h1 id=&quot;running-a-pod&quot;&gt;Running a pod&lt;/h1&gt;

&lt;p&gt;There are a few ways the kubelet finds pods to run:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a directory it polls for new pod manifests to run&lt;/li&gt;
  &lt;li&gt;a URL it polls and downloads pod manifests from&lt;/li&gt;
  &lt;li&gt;from the Kubernetes API server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first of these is definitely the simplest: to run a pod, we just put a
manifest file in the watched directory. Every 20 seconds, the kubelet checks
for changes in the directory, and adjusts what it’s running based on what it
finds.  This means both launching pods that are added, as well as killing ones
that are removed.&lt;/p&gt;

&lt;p&gt;The kubelet is such a low level component with such limited responsibilities
that we can actually use it independently of Kubernetes—all we have to do is
not tell it about a Kubernetes API server. The kubelet supports &lt;a href=&quot;https://github.com/docker/docker&quot;&gt;Docker&lt;/a&gt; and
&lt;a href=&quot;https://github.com/coreos/rkt&quot;&gt;rkt&lt;/a&gt; as continer runtimes. The default is Docker, and that’s what we’ll use in
the examples here. You’ll need a machine with Docker installed and running to
try this out.&lt;/p&gt;

&lt;p&gt;First off, let’s get the kubelet binary from Google.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://storage.googleapis.com/kubernetes-release/release/v1.0.3/bin/linux/amd64/kubelet
$ chmod +x kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you run &lt;code&gt;./kubelet --help&lt;/code&gt;, you’ll get an overwhelming list of options. For
what we’re about to do, we only need one of them though: the &lt;code&gt;--config&lt;/code&gt; option.
This is the directory that the kubelet will watch for pod manifests to run.
We’ll create a directory for this, and then start the kubelet. You might need
to run it under &lt;code&gt;sudo&lt;/code&gt; so that it can talk to the docker daemon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir manifests
$ ./kubelet --config=$PWD/manifests
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s stick the example nginx pod manifest from above in an &lt;code&gt;nginx.yaml&lt;/code&gt;
file, and then drop it in the &lt;code&gt;manifests&lt;/code&gt; directory. After a short wait, the
kubelet will notice the file and fire up the pod.&lt;/p&gt;

&lt;p&gt;We can check the list of running containers with &lt;code&gt;docker ps&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID        IMAGE                                  COMMAND                CREATED             STATUS              PORTS               NAMES
f1a27680e401        busybox:latest                         &quot;/bin/sh -c &#39;while t   6 seconds ago       Up 5 seconds                            k8s_log-truncator.72cfff7a_nginx-kx_default_419bc51e985b6bb5e53ea305e2c1e737_401a4c94   
c5e357fc981a        nginx:latest                           &quot;nginx -g &#39;daemon of   6 seconds ago       Up 6 seconds                            k8s_nginx.515d0778_nginx-kx_default_419bc51e985b6bb5e53ea305e2c1e737_cd02602b           
b2692643c372        gcr.io/google_containers/pause:0.8.0   &quot;/pause&quot;               6 seconds ago       Up 6 seconds                            k8s_POD.ef28e851_nginx-kx_default_419bc51e985b6bb5e53ea305e2c1e737_836cadc7             
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are three containers running: the &lt;code&gt;nginx&lt;/code&gt; and &lt;code&gt;log-truncator&lt;/code&gt; containers
we defined, as well as the pod infrastructure container.&lt;sup id=&quot;fnref:pause&quot;&gt;&lt;a href=&quot;#fn:pause&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The
infrastructure container is where the kubelet puts all the resources that are
shared across containers in the pod. This includes the IP, as well as any
volumes we’ve defined. We can poke around with &lt;code&gt;docker inspect&lt;/code&gt; to see how
they’re configured and hooked up to each other:&lt;/p&gt;

&lt;!-- annoying quoting needed because liquid templates use  for something --&gt;
&lt;pre&gt;&lt;code&gt;$ docker inspect --format &#39;{{ .NetworkSettings.IPAddress  }}&#39; f1a27680e401

$ docker inspect --format &#39;{{ .NetworkSettings.IPAddress  }}&#39; c5e357fc981a

$ docker inspect --format &#39;{{ .NetworkSettings.IPAddress  }}&#39; b2692643c372
172.17.0.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nginx and log trunctator containers have no IP, but the infrastructure container does. Taking a
closer look at at the containers we defined, we can see their &lt;code&gt;NetworkMode&lt;/code&gt; is set to
use the infrastructure container’s network:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker inspect --format &#39;{{ .HostConfig.NetworkMode  }}&#39; c5e357fc981a
container:b2692643c37216c3f1650b4a5b96254270e0489b96c022c9873ad63c4809ce93
$ docker inspect --format &#39;{{ .HostConfig.NetworkMode  }}&#39; f1a27680e401
container:b2692643c37216c3f1650b4a5b96254270e0489b96c022c9873ad63c4809ce93
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since we exposed port 80 from the nginx container with &lt;code&gt;containerPort&lt;/code&gt;,
we can connect to the nginx server at port 80 at the pod’s IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://172.17.0.2 | head -4
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It really is running! And just to check the log truncator is doing what we
expect, let’s watch the log file and make some requests with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker exec -tty f1a27680e401 watch cat /logdir/access.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;while we make a few requests. The log lines accumulate for a bit, but then they
all disappear: the truncator doing its job!&lt;/p&gt;

&lt;h1 id=&quot;kubelet-introspection&quot;&gt;Kubelet introspection&lt;/h1&gt;

&lt;p&gt;The kubelet also has an internal HTTP server. We won’t go into it in detail,
except to say that it serves a read-only view at port
10255.  There’s a health check endpoint at &lt;code&gt;/healthz&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://localhost:10255/healthz
ok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are also a few status endpoints. For example, you can get a list
of running pods at &lt;code&gt;/pods&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null http://localhost:10255/pods | jq . | head
{
  &quot;kind&quot;: &quot;PodList&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {},
  &quot;items&quot;: [
    {
      &quot;metadata&quot;: {
        &quot;name&quot;: &quot;nginx-kx&quot;,
        &quot;namespace&quot;: &quot;default&quot;,
        &quot;selfLink&quot;: &quot;/api/v1/pods/namespaces/nginx-kx/default&quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also get specs of the machine the kubelet is running on at
&lt;code&gt;/spec/&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --stderr /dev/null  http://localhost:10255/spec/ | jq . | head
{
  &quot;num_cores&quot;: 4,
  &quot;cpu_frequency_khz&quot;: 2700000,
  &quot;memory_capacity&quot;: 4051689472,
  &quot;machine_id&quot;: &quot;9eacc5220f4b41e0a22972d8a47ccbe1&quot;,
  &quot;system_uuid&quot;: &quot;818B908B-D053-CB11-BC8B-EEA826EBA090&quot;,
  &quot;boot_id&quot;: &quot;a95a337d-6b54-4359-9a02-d50fb7377dd1&quot;,
  &quot;filesystems&quot;: [
    {
      &quot;device&quot;: &quot;/dev/mapper/kx--vg-root&quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;tearing-things-down&quot;&gt;Tearing things down&lt;/h1&gt;

&lt;p&gt;Finally, we can clean up after ourselves. Just deleting the nginx pod
manifest will result in the kubelet stopping the containers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm $PWD/manifests/nginx.yaml
$ sleep 20  # wait for the kublet to spot the removed manifest
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
$ curl --stderr /dev/null http://localhost:10255/pods
{&quot;kind&quot;:&quot;PodList&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;metadata&quot;:{},&quot;items&quot;:null}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All gone!&lt;/p&gt;

&lt;p&gt;We’ve seen that while the kubelet is a part of Kubernetes, at heart it’s a
container-oriented process watcher. You can use it in isolation to manage
containers running on a single host. In fact, the Kubernetes &lt;a href=&quot;http://kubernetes.io/v1.0/docs/getting-started-guides/docker.html#step-two-run-the-master&quot;&gt;getting started
guides for Docker&lt;/a&gt; run the kubelet under Docker and
use the kubelet to manage the Kubernetes master components. In a later post,
we’ll do something similar!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:pause&quot;&gt;

      &lt;p&gt;The &lt;code&gt;pause&lt;/code&gt; command that the infrastructure container runs is a 129 byte
ELF binary that just calls the &lt;a href=&quot;http://man7.org/linux/man-pages/man2/pause.2.html&quot;&gt;&lt;code&gt;pause&lt;/code&gt; system call&lt;/a&gt;, and
exits when a signal is received. This keeps the infrastructure container
around until the kubelet brings it down. It’s pretty cool, check the
&lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/88317efb42db763b9fb97cd1d9ac1465e62009d0/third_party/pause/pause.asm&quot;&gt;source&lt;/a&gt;! &lt;a href=&quot;#fnref:pause&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 27 Aug 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/08/27/what-even-is-a-kubelet/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/08/27/what-even-is-a-kubelet/</guid>
        
        
      </item>
    
      <item>
        <title>Recurse Center lab notes catch up</title>
        <description>&lt;p&gt;I’m way behind on posting here. Here is a quick rundown of what I’ve been up to.&lt;/p&gt;

&lt;p&gt;The biggest thing is that I managed to run &lt;a href=&quot;http://kythe.io/&quot;&gt;Kythe&lt;/a&gt;! It took me half a week to
get it to work with a hello world program, but once I had that I was good to
go. This meant I got to play with &lt;a href=&quot;http://bazel.io/&quot;&gt;Bazel&lt;/a&gt; again, which was nicely familiar.&lt;/p&gt;

&lt;p&gt;A lot of the confusion was that remote Bazel repositories don’t quite work for
C++ in the current state. My approach of referring to the Kythe indexer from a
Bazel repository with my hello world program just wasn’t going to work, even
though I kept trying. In the end, I just copied the binaries into my tree and
set up the configuration needed to run them.&lt;/p&gt;

&lt;p&gt;I gave a demo of the web UI that comes with Kythe, run against some data
structure code that had Alice written. There’s a somewhat hilarious blowup in
size: 7 kB of source results in a couple of serving database tables. I also did
a hackish port of the SQLite build over to Bazel so I could run Kythe over it.
The 1.5 MB of source resulted in a ~80 MB database. The flipside is that the
API server responds to most queries in a few milliseconds.&lt;/p&gt;

&lt;p&gt;Keeping with Kythe, I’m currently working on an alternative web UI. The
provided one is really good to understand how nodes are linked together in the
graph structure that Kythe builds up, but it’s too noisy and clunky to use as a
day-to-day code exploration tool. As an example, it shows the syntactic
relationship of a variable reference being a child of the function it occurs
in.&lt;/p&gt;

&lt;p&gt;I’m pretty excited about this project because one of the things I wanted to do
at RC was to learn JavaScript (well, ECMAScript 2015). The &lt;a href=&quot;https://github.com/google/kythe/tree/master/kythe/web/ui/src-cljs/ui&quot;&gt;Kythe web
UI&lt;/a&gt; is in &lt;a href=&quot;https://github.com/clojure/clojurescript&quot;&gt;Clojurescript&lt;/a&gt; using &lt;a href=&quot;https://github.com/omcljs/om&quot;&gt;Om&lt;/a&gt;, which I’m not currently
interested in learning, so I’ll be starting my own from scratch.&lt;/p&gt;

&lt;p&gt;Just to keep things slightly weird, I’m using &lt;a href=&quot;https://github.com/Reactive-Extensions/RxJS&quot;&gt;RxJS&lt;/a&gt;, the Reactive Extensions
for JavaScript. I’m trying to emulate the architecture used in &lt;a href=&quot;http://elm-lang.org/&quot;&gt;Elm&lt;/a&gt; in place
of the &lt;a href=&quot;http://facebook.github.io/flux/&quot;&gt;Flux&lt;/a&gt; architecture that’s more commonly used with React. The &lt;a href=&quot;https://github.com/evancz/elm-architecture-tutorial/&quot;&gt;Elm
architecture&lt;/a&gt; is pleasantly functional, consisting of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a model type &lt;code&gt;Model&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;an action type &lt;code&gt;Action&lt;/code&gt; that specifies all the kinds of actions that exist;&lt;/li&gt;
  &lt;li&gt;an update function &lt;code&gt;update :: Action -&amp;gt; Model -&amp;gt; Model&lt;/code&gt;; and&lt;/li&gt;
  &lt;li&gt;a view function &lt;code&gt;view :: Model -&amp;gt; Html&lt;/code&gt; that renders the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all wired up with functional reactive stuff—&lt;a href=&quot;http://package.elm-lang.org/packages/elm-lang/core/2.1.0/Signal&quot;&gt;Signals&lt;/a&gt; in Elm,
&lt;a href=&quot;https://github.com/Reactive-Extensions/RxJS/blob/master/doc/api/core/observable.md&quot;&gt;Observables&lt;/a&gt; in RxJS. The user input events are translated into a stream of
actions. A stream of models is created by folding the update function over the
action stream. Finally, the view is kept up to date by mapping the view
function over the model stream.&lt;/p&gt;

&lt;p&gt;I have no real idea what I’m doing, so I’m starting off with translating the
&lt;a href=&quot;https://github.com/evancz/elm-todomvc&quot;&gt;Elm TodoMVC&lt;/a&gt; implementation to this cobbled together set of
tools. This should be done fairly soon, then I’ll go back to the Kythe code
browser.&lt;/p&gt;

</description>
        <pubDate>Wed, 29 Jul 2015 00:01:49 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/07/29/recurse-center-lab-notes-catch-up/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/07/29/recurse-center-lab-notes-catch-up/</guid>
        
        
      </item>
    
      <item>
        <title>Preludes</title>
        <description>&lt;p&gt;This evening I walked from the Lincoln Center at 66th and Broadway down
to 40th street, all the while listening to Rachmaninoff’s 1929
performance of his Piano Concerto № 2. My noise cancelling headphones
masked the never ending cacophony of the city. The experience of the
third movement was slightly surreal, coinciding with my traversal of
Times Square.&lt;/p&gt;

&lt;p&gt;This was brought on by having just watched &lt;a href=&quot;http://www.lct.org/shows/preludes/&quot;&gt;Preludes&lt;/a&gt;, ‘a musical fantasia
set in the hypnotized mind of Russian composer Sergei Rachmaninoff’,
which is playing at the Lincoln Center through to August second.
Rachmaninoff suffered depression and writer’s block, and visited
hypnotherapist Nikolai Dahl to help him break through it. Rachmaninoff
dedicated the second Concerto to Dahl.&lt;/p&gt;

&lt;p&gt;To say I really enjoyed the play would be a slight understatement. One
of the actors was at a grand piano in center stage. He played pieces
that came up in dialogue, as well as snatches of themes from Piano
Concerto № 2 as the composer worked through his block. I experienced
&lt;a href=&quot;http://www.bbc.com/future/story/20150721-when-was-the-last-time-music-gave-you-a-skin-orgasm&quot;&gt;musical frisson—or ‘skin orgasm’—&lt;/a&gt;several times; I hadn’t
listened to any Rachmaninoff in a good while, but he is way up on my
list of favourite composers. The C♯ minor and G minor preludes, both of
which I love, were played in full.&lt;/p&gt;

&lt;p&gt;Musical performances aside, there was plenty of food for thought.
Creativity, performance anxiety, writer’s block, depression, a need to
be ‘great’, obsession with how others perceive you and your work—all too
familiar! I wish I’d had a notebook out to jot down some quotes to think
over more, especially as I come to the last three weeks at the Recurse
Center. I’ve not written here in about two weeks, and some of that is
not feeling like I have something worthy of writing about. And the
longer I go without writing, the more important it seems for the next
writing to be, if not great, at least good. I hope the play will snap me
out of that. Let’s see!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to David Albert for telling me about the play, and to the fellow
recursers who joined me at the play.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/07/26/preludes/</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/07/26/preludes/</guid>
        
        
      </item>
    
      <item>
        <title>Recurse Center lab notes 2015-07-15: X.509 PKI</title>
        <description>&lt;p&gt;I spent today working towards my Kubernetes cluster. On the way, I decided I
wanted to have client certificates to control access to the API server.&lt;/p&gt;

&lt;p&gt;Here’s an overview of where I’m going:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;generate a root certificate and key, ideally stored offline and in a hardware
security module (HSM)&lt;/li&gt;
  &lt;li&gt;use that to sign an intermediate root certificate&lt;/li&gt;
  &lt;li&gt;use the intermediate root to sign any client or server certificates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m just tinkering for now, and so I’m storing the root certificate on my
laptop. I’m using easy-rsa to generate the setup. It’s a collection of
scripts from the OpenVPN project to help set up a CA to use for VPN
authentication.&lt;/p&gt;

&lt;p&gt;I’m roughly following these guides, and doing peripheral reading up on what I’m
actually doing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openvpn.net/index.php/open-source/documentation/miscellaneous/77-rsa-key-management.html&quot;&gt;https://openvpn.net/index.php/open-source/documentation/miscellaneous/77-rsa-key-management.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wiki.archlinux.org/index.php/Create_a_Public_Key_Infrastructure_Using_the_easy-rsa_Scripts&quot;&gt;https://wiki.archlinux.org/index.php/Create_a_Public_Key_Infrastructure_Using_the_easy-rsa_Scripts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A fun part is I get to name my CA. I went with ‘Kamal Marhubi CA’, but I’m
probably going to redo it. I also have to get a better handle on all these &lt;code&gt;O&lt;/code&gt;,
&lt;code&gt;CN&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, and other fields that show up in an X.509 certificate, and what’s
allowed there. &lt;a href=&quot;https://tools.ietf.org/html/rfc5280&quot;&gt;RFC 5280&lt;/a&gt; is my friend here.&lt;/p&gt;

&lt;p&gt;Once done, I’ll have to keep running these scipts whenever I need new
certificates. That’s just fine for my small use case! I’ll need a server
certificate for each of the API server nodes, client certificates for each
worker node, and a client certificate for me to connect to the API server.&lt;/p&gt;

&lt;p&gt;If you want to get fancy, CloudFlare have &lt;a href=&quot;https://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure/&quot;&gt;a good post on setting up fancier
PKI&lt;/a&gt; using their &lt;a href=&quot;https://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure/&quot;&gt;CFSSL&lt;/a&gt; project. It includes an API server that can
issue certificates automatically.  This would be useful if you wanted your
services to be authenticated, but also to be able to spin new instances up and
down dynamically.&lt;/p&gt;

&lt;p&gt;The CloudFlare post talks about using multiple intermediate roots for different
services. The example is that an API server only trusts certificates signed by
the DB root CA, and vice versa. A comment there mentions using &lt;a href=&quot;https://tools.ietf.org/html/rfc5280#section-4.2.1.12&quot;&gt;Extended Key
Usage (EKU)&lt;/a&gt; instead. This is a standard way to designate the type of use
a certificate is valid for. This requires an OID. For external use, you’d have
to register one. But in this use case, you can use aUUID based OID. Some more
info for my future self: &lt;a href=&quot;http://www.itu.int/en/ITU-T/asn1/Pages/UUID/uuids.aspx&quot;&gt;one&lt;/a&gt;, &lt;a href=&quot;http://www.oid-info.com/get/2.25&quot;&gt;two&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For maximal fanciness, you want to be using HSMs and TPMs where possible. Both
are special hardware that stores keys, and is able to do cryptographic
operations without the key ever leaving the device. You probably have a TPM in
the computer you’re using right now, though it might be disabled. I enabled
mine in the BIOS setup, and have ‘taken ownership’ of it. As in, I literally
ran a command called &lt;code&gt;tpm_takeownership&lt;/code&gt;. I’m going to see if I can get it
added as a security device in Firefox. That way, when the time comes, I’ll be
able to store my client certificates in there. Or something!&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015-07-15/recurse-center-lab-notes</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015-07-15/recurse-center-lab-notes</guid>
        
        
      </item>
    
      <item>
        <title>Rethinking my rules</title>
        <description>&lt;p&gt;After a conversation with &lt;a href=&quot;http://jvns.ca/&quot;&gt;Julia&lt;/a&gt; and &lt;a href=&quot;https://blog.gregbrockman.com/&quot;&gt;Greg&lt;/a&gt; about how to approach the
Recurse Center, I’m revisiting my not-at-RC project list. I wrote about
this early on in &lt;a href=&quot;http://kamalmarhubi.com/blog/2015/06/02/goals-non-goals-and-anti-goals/&quot;&gt;Goals, non-goals, and anti-goals&lt;/a&gt;. The
gist of it is that I wanted to avoid spending time on things that were
mostly ‘busywork and configuration’. The list of not-at-RC projects was
vaguely:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;, &lt;a href=&quot;https://coreos.com/&quot;&gt;CoreOS&lt;/a&gt;, and other containery and clustery stuff&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bazel.io/&quot;&gt;Bazel&lt;/a&gt;, Google’s recently open sourced build tool&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kythe.io/&quot;&gt;Kythe&lt;/a&gt;, Google’s recently open sourced source code indexer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;, a monitoring and alerting system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s a common thread here of wanting a Google-away-from-Google, at
least in terms of the developer experience. This might be misguided,
because not all problems are Google-sized. The developer experience of a
great code search, unified build system, and cluster scheduler is really
great though! The main problem is I’m not sure how many of these things
require scale to be worthwhile.&lt;/p&gt;

&lt;p&gt;Back to the conversation that made me think of changing my rules. Greg
said it was valuable to explore and learn to use tools to give you a
better understanding of how to judge similar things. That resonated with
me: if I want to have informed opinions on things in this space, I have
to use some of them!&lt;/p&gt;

&lt;p&gt;With all that in mind, here’s what I’m working on at the moment:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fixed an issue I’d been having with my CoreOS cluster, so that I no longer
get complaints about failed systemd units on login&lt;/li&gt;
  &lt;li&gt;reading up on what’s changed in Kubernetes since I read the original design
docs when it was first released. There’s a lot of new concepts in there now,
and they’re &lt;a href=&quot;http://kuberneteslaunch.com/&quot;&gt;gearing up for a 1.0 next week&lt;/a&gt;&lt;sup id=&quot;fnref:launch-site&quot;&gt;&lt;a href=&quot;#fn:launch-site&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;,
so this seems like a good time to dig in!&lt;/li&gt;
  &lt;li&gt;I installed &lt;a href=&quot;https://sandstorm.io/&quot;&gt;Sandstorm&lt;/a&gt; on a new server in GCE. Eventually I plan to move
this to the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course, bringing in a new theme means something has to go to make room. I’m
probably going to drop the priority on learning JavaScript / ES15. Unlike with
the systems programming I was doing, I don’t have a strong and clear goal in
mind there. I do expect to get back to &lt;code&gt;mmap&lt;/code&gt; and my other friends in that
world though. I’ll treat Kubernetes and CoreOS as a little break for now, and
then figure out a way to interleave the different kinds of work.&lt;/p&gt;

&lt;p&gt;This should be fun!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:launch-site&quot;&gt;

      &lt;p&gt;Yes, apparently they got a domain just for the launch. &lt;a href=&quot;#fnref:launch-site&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 14 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/07/14/rethinking-my-rules</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/07/14/rethinking-my-rules</guid>
        
        
      </item>
    
      <item>
        <title>Recurse Center lab notes 2015-07-08</title>
        <description>&lt;p&gt;Yesterday’s tinkering with Bazel and Kythe seeped into today. I attempted to
run Kythe’s indexers on its own code base. This was a multi-stage process that
took something like 15 hours of CPU time and generated 90+ GB of intermediate
data. The end result was disappointing: I get a 500 error in the provided web
service.&lt;/p&gt;

&lt;p&gt;It was definitely fun to use Blaze / Bazel again though! While Kythe didn’t
work out, I’m vaguely tempted to use Bazel on some personal stuff, but it needs
more build rules before that becomes non-painful. I need to take a slightly
closer look at what is available already. It looked like someone was adding
build rules for Rust…&lt;/p&gt;

&lt;p&gt;The rest of the day was taken up by being non-productive, followed by doing
some more ES6, followed by napping for too long. We’ll try again tomorrow!&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/07/08/recurse-center-lab-notes</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/07/08/recurse-center-lab-notes</guid>
        
        
      </item>
    
      <item>
        <title>Recurse Center lab notes 2015-07-07: back to mmap; other people&#39;s code; things I&#39;m not meant to do while at RC</title>
        <description>&lt;p&gt;I went back to &lt;code&gt;mmap&lt;/code&gt; today. I am finally attempting to put together what I
picked up from experimenting in the last while towards writing a memory mapped
file message builder for Cap’n Proto. Current status:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I’ve created a new class with a constructor &lt;code&gt;MmapMessageBuilder(int
fd)&lt;/code&gt; that maps the file descriptor&lt;/li&gt;
  &lt;li&gt;Things mostly magically work because I’m supplying an alternate
implementation of an interface&lt;/li&gt;
  &lt;li&gt;I am not yet handling the resizing of the backing file in any way&lt;/li&gt;
  &lt;li&gt;I am not yet including a segment table at the front of the file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last one means I can’t get the files read back in with the existing
deserializers. I was totally able to see it in the bytes though!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;00000000  00 00 00 00 00 00 01 00  01 00 00 00 57 00 00 00  |............W...|
00000010  08 00 00 00 01 00 04 00  7b 00 00 00 02 00 00 00  |........{.......|
00000020  21 00 00 00 32 00 00 00  21 00 00 00 92 00 00 00  |!...2...!.......|
00000030  29 00 00 00 17 00 00 00  39 00 00 00 22 00 00 00  |).......9...&quot;...|
00000040  c8 01 00 00 00 00 00 00  35 00 00 00 22 00 00 00  |........5...&quot;...|
00000050  35 00 00 00 82 00 00 00  39 00 00 00 27 00 00 00  |5.......9...&#39;...|
00000060  00 00 00 00 00 00 00 00  41 6c 69 63 65 00 00 00  |........Alice...|
00000070  61 6c 69 63 65 40 65 78  61 6d 70 6c 65 2e 63 6f  |alice@example.co|
00000080  6d 00 00 00 00 00 00 00  04 00 00 00 01 00 01 00  |m...............|
00000090  00 00 00 00 00 00 00 00  01 00 00 00 4a 00 00 00  |............J...|
000000a0  35 35 35 2d 31 32 31 32  00 00 00 00 00 00 00 00  |555-1212........|
000000b0  4d 49 54 00 00 00 00 00  42 6f 62 00 00 00 00 00  |MIT.....Bob.....|
000000c0  62 6f 62 40 65 78 61 6d  70 6c 65 2e 63 6f 6d 00  |bob@example.com.|
000000d0  08 00 00 00 01 00 01 00  01 00 00 00 00 00 00 00  |................|
000000e0  09 00 00 00 4a 00 00 00  02 00 00 00 00 00 00 00  |....J...........|
000000f0  09 00 00 00 4a 00 00 00  35 35 35 2d 34 35 36 37  |....J...555-4567|
00000100  00 00 00 00 00 00 00 00  35 35 35 2d 37 36 35 34  |........555-7654|
00000110  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
*
00001000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Writing out the segment table mostly requires trawling through someone else’s
code base. The Cap’n Proto code is fairly clear and well documented, especially
the user interfaces. Some key innards are not as well documented, however. In
particular, the &lt;a href=&quot;https://github.com/sandstorm-io/capnproto/blob/1702050903e1038acf0556c6eabcf8f99702690d/c%2B%2B/src/capnp/arena.h#L197&quot;&gt;arena abstraction&lt;/a&gt; used to look after the
allocated segments could do with a short overview. I’m most of the way to
understanding how it fits together, but it’s been slower than I would have
liked.&lt;/p&gt;

&lt;p&gt;The process of understanding the code has been slowed by needing to learn the
&lt;a href=&quot;https://github.com/sandstorm-io/capnproto/tree/master/c%2B%2B/src/kj&quot;&gt;&lt;code&gt;kj&lt;/code&gt;&lt;/a&gt; library that Cap’n Proto makes heavy use of. I’m trying to find out
the boundaries of what &lt;code&gt;kj&lt;/code&gt; aims to be, but so far it looks to include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;replacements for some standard library classes, eg, &lt;a href=&quot;https://github.com/sandstorm-io/capnproto/blob/1702050903e1038acf0556c6eabcf8f99702690d/c%2B%2B/src/kj/memory.h#L99&quot;&gt;&lt;code&gt;kj::Own&lt;/code&gt;&lt;/a&gt;
is used where you might use&lt;a href=&quot;http://en.cppreference.com/w/cpp/memory/unique_ptr&quot;&gt; &lt;code&gt;std::unique_ptr&lt;/code&gt;&lt;/a&gt;, and
there is a separate &lt;a href=&quot;https://github.com/sandstorm-io/capnproto/blob/master/c%2B%2B/src/kj/io.h&quot;&gt;stream abstraction&lt;/a&gt; that is used instead of
&lt;a href=&quot;http://en.cppreference.com/w/cpp/io&quot;&gt;the standard one&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;an event loop—the C++ standard library still has nothing here; it may
in 2017 though!&lt;/li&gt;
  &lt;li&gt;some templated array classes that seem to be a pointer-length pair;
there is some macro magic to allow allocating such an array on the
stack&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An overview of this library would also be really nice to have. My life
would be a bit easier if this was all STL stuff.&lt;/p&gt;

&lt;p&gt;All this frustration has me trying to use Google’s &lt;a href=&quot;http://www.kythe.io/&quot;&gt;Kythe code indexing
project&lt;/a&gt; on Cap’n Proto. This in turn has me messing around with their
&lt;a href=&quot;http://bazel.io/&quot;&gt;Bazel build tool&lt;/a&gt;.  Both of these tools are up there on my
not-while-at-RC list, but I’m giving myself a very short pass to see if I can
port the build over.  Building these tools is pretty slow going…&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;current status: waiting for llvm to compile :/&lt;/p&gt;&amp;mdash; Kamal Marhubi (@kamalmarhubi) &lt;a href=&quot;https://twitter.com/kamalmarhubi/status/618613903593996289&quot;&gt;July 8, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Tue, 07 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://kamalmarhubi.com/blog/2015/07/07/recurse-center-lab-notes</link>
        <guid isPermaLink="true">http://kamalmarhubi.com/blog/2015/07/07/recurse-center-lab-notes</guid>
        
        
      </item>
    
  </channel>
</rss>
